{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_2_Object_Detection_using_YOLOv3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMmrvOg896pdQxOcl3JVl+R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ssV-y65Ph7nZ"},"source":["# How to Perform Object Detection With YOLOv3 in Keras\n","\n","This tutorial is based on the following website: \n","https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n"]},{"cell_type":"markdown","metadata":{"id":"TLrn_zh6zAQo"},"source":["---\n","### Mount to Google Drive\n","Mount to your Google Drive to access the images and model files."]},{"cell_type":"code","metadata":{"id":"5AK31II_h1Zm"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","dataset_location = \"/content/drive/My Drive/Crafting/ADLCV/dataset/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HddY5wN8-lqF"},"source":["---\n","### Import of libraries"]},{"cell_type":"code","metadata":{"id":"Q_MWDe8x-lIh"},"source":["import numpy as np\n","from numpy import expand_dims\n","from keras.models import load_model\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from matplotlib import pyplot\n","from matplotlib.patches import Rectangle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Nzf5XM9zIf2"},"source":["---\n","### Load the Keras-Yolov3 model\n","Refer to the website on the creation of this model file."]},{"cell_type":"code","metadata":{"id":"QTOrxFBy-j1t"},"source":["# load yolov3 model\n","model = load_model(dataset_location+'yolo/yolov3_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVnBcfL2zJZe"},"source":["---\n","### A function to load and pre-process the image"]},{"cell_type":"code","metadata":{"id":"OFwAepj0_HsC"},"source":["# load and prepare an image\n","def load_image_pixels(filename, shape):\n","    # load the image to get its shape\n","    image = load_img(filename)\n","    width, height = image.size\n","\n","    # load the image with the required size\n","    image = load_img(filename, target_size=shape)\n","    # convert to numpy array\n","    image = img_to_array(image)\n","    # scale pixel values to [0, 1]\n","    image = image.astype('float32')\n","    image /= 255.0\n","    # add a dimension so that we have one sample\n","    image = expand_dims(image, 0)\n","    \n","    return image, width, height"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jp8_fRZm_T-c"},"source":["---\n","### Load an image and make a prediction."]},{"cell_type":"code","metadata":{"id":"JXFLfbW2_lPT"},"source":["# define the expected input shape for the model\n","input_w, input_h = 416, 416\n","# define our new photo\n","photo_filename = dataset_location+'yolo/zebra.jpg'\n","# load and prepare image\n","image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n","\n","# make prediction\n","yhat = model.predict(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdburE9vBEVj"},"source":["---\n","### Decoding the prediction\n","The output of the model is, in fact, encoded candidate bounding boxes from three different grid sizes, and the boxes are defined the context of anchor boxes, carefully chosen based on an analysis of the size of objects in the MSCOCO dataset.\n","\n","The script provided by experiencor provides a function called decode_netout() that will take each one of the NumPy arrays, one at a time, and decode the candidate bounding boxes and class predictions. Further, any bounding boxes that donâ€™t confidently describe an object (e.g. all class probabilities are below a threshold) are ignored. We will use a probability of 60% or 0.6. The function returns a list of BoundBox instances that define the corners of each bounding box in the context of the input image shape and class probabilities."]},{"cell_type":"code","metadata":{"id":"7hLf1hm6V0yN"},"source":["class BoundBox:\n","\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n","\t\tself.xmin = xmin\n","\t\tself.ymin = ymin\n","\t\tself.xmax = xmax\n","\t\tself.ymax = ymax\n","\t\tself.objness = objness\n","\t\tself.classes = classes\n","\t\tself.label = -1\n","\t\tself.score = -1\n","\n","\tdef get_label(self):\n","\t\tif self.label == -1:\n","\t\t\tself.label = np.argmax(self.classes)\n","\n","\t\treturn self.label\n","\n","\tdef get_score(self):\n","\t\tif self.score == -1:\n","\t\t\tself.score = self.classes[self.get_label()]\n","\n","\t\treturn self.score\n","\n","def _sigmoid(x):\n","\treturn 1. / (1. + np.exp(-x))\n","\n","def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n","\tgrid_h, grid_w = netout.shape[:2]\n","\tnb_box = 3\n","\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n","\tnb_class = netout.shape[-1] - 5\n","\tboxes = []\n","\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n","\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n","\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n","\tnetout[..., 5:] *= netout[..., 5:] > obj_thresh\n","\n","\tfor i in range(grid_h*grid_w):\n","\t\trow = i / grid_w\n","\t\tcol = i % grid_w\n","\t\tfor b in range(nb_box):\n","\t\t\t# 4th element is objectness score\n","\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n","\t\t\tif(objectness.all() <= obj_thresh): continue\n","\t\t\t# first 4 elements are x, y, w, and h\n","\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n","\t\t\tx = (col + x) / grid_w # center position, unit: image width\n","\t\t\ty = (row + y) / grid_h # center position, unit: image height\n","\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n","\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n","\t\t\t# last elements are class probabilities\n","\t\t\tclasses = netout[int(row)][col][b][5:]\n","\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n","\t\t\tboxes.append(box)\n","\treturn boxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-K6u7PZV9CN"},"source":["# define the anchors\n","anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n","# define the probability threshold for detected objects\n","class_threshold = 0.6\n","boxes = list()\n","for i in range(len(yhat)):\n","\t# decode the output of the network\n","\tboxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYYPf96eWDBU"},"source":["Next, the bounding boxes can be stretched back into the shape of the original image. This is helpful as it means that later we can plot the original image and draw the bounding boxes, hopefully detecting real objects.\n","\n","The experiencor script provides the correct_yolo_boxes() function to perform this translation of bounding box coordinates, taking the list of bounding boxes, the original shape of our loaded photograph, and the shape of the input to the network as arguments. The coordinates of the bounding boxes are updated directly."]},{"cell_type":"code","metadata":{"id":"hpRLEO7PWJZk"},"source":["def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n","\tnew_w, new_h = net_w, net_h\n","\tfor i in range(len(boxes)):\n","\t\tx_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n","\t\ty_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n","\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n","\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n","\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n","\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzgPsxkYWTnU"},"source":["# correct the sizes of the bounding boxes for the shape of the image\n","correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xYpSuKiGWW-k"},"source":["The model has predicted a lot of candidate bounding boxes, and most of the boxes will be referring to the same objects. The list of bounding boxes can be filtered and those boxes that overlap and refer to the same object can be merged. We can define the amount of overlap as a configuration parameter, in this case, 50% or 0.5. This filtering of bounding box regions is generally referred to as non-maximal suppression and is a required post-processing step.\n","\n","The experiencor script provides this via the do_nms() function that takes the list of bounding boxes and a threshold parameter. Rather than purging the overlapping boxes, their predicted probability for their overlapping class is cleared. This allows the boxes to remain and be used if they also detect another object type."]},{"cell_type":"code","metadata":{"id":"AsGl878_WcVr"},"source":["def _interval_overlap(interval_a, interval_b):\n","\tx1, x2 = interval_a\n","\tx3, x4 = interval_b\n","\tif x3 < x1:\n","\t\tif x4 < x1:\n","\t\t\treturn 0\n","\t\telse:\n","\t\t\treturn min(x2,x4) - x1\n","\telse:\n","\t\tif x2 < x3:\n","\t\t\t return 0\n","\t\telse:\n","\t\t\treturn min(x2,x4) - x3\n","\n","def bbox_iou(box1, box2):\n","\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n","\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n","\tintersect = intersect_w * intersect_h\n","\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n","\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n","\tunion = w1*h1 + w2*h2 - intersect\n","\treturn float(intersect) / union\n","\n","def do_nms(boxes, nms_thresh):\n","\tif len(boxes) > 0:\n","\t\tnb_class = len(boxes[0].classes)\n","\telse:\n","\t\treturn\n","\tfor c in range(nb_class):\n","\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n","\t\tfor i in range(len(sorted_indices)):\n","\t\t\tindex_i = sorted_indices[i]\n","\t\t\tif boxes[index_i].classes[c] == 0: continue\n","\t\t\tfor j in range(i+1, len(sorted_indices)):\n","\t\t\t\tindex_j = sorted_indices[j]\n","\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n","\t\t\t\t\tboxes[index_j].classes[c] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRuPZqdwWx2O"},"source":["# suppress non-maximal boxes\n","do_nms(boxes, 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vBpDXeRW-Ls"},"source":["This will leave us with the same number of boxes, but only very few of interest. We can retrieve just those boxes that strongly predict the presence of an object: that is are more than 60% confident. This can be achieved by enumerating over all boxes and checking the class prediction values. We can then look up the corresponding class label for the box and add it to the list. Each box must be considered for each class label, just in case the same box strongly predicts more than one object.\n","\n","We can develop a get_boxes() function that does this and takes the list of boxes, known labels, and our classification threshold as arguments and returns parallel lists of boxes, labels, and scores."]},{"cell_type":"code","metadata":{"id":"t2dqH7XgXM-c"},"source":["# get all of the results above a threshold\n","def get_boxes(boxes, labels, thresh):\n","\tv_boxes, v_labels, v_scores = list(), list(), list()\n","\t# enumerate all boxes\n","\tfor box in boxes:\n","\t\t# enumerate all possible labels\n","\t\tfor i in range(len(labels)):\n","\t\t\t# check if the threshold for this label is high enough\n","\t\t\tif box.classes[i] > thresh:\n","\t\t\t\tv_boxes.append(box)\n","\t\t\t\tv_labels.append(labels[i])\n","\t\t\t\tv_scores.append(box.classes[i]*100)\n","\t\t\t\t# don't break, many labels may trigger for one box\n","\treturn v_boxes, v_labels, v_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dCxpye_uXP-F"},"source":["# define the labels\n","labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n","    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n","    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n","    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n","    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n","    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n","    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n","    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n","    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n","    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n","# get the details of the detected objects\n","v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2BrFuqMdXfCT"},"source":["We can also plot our original photograph and draw the bounding box around each detected object. This can be achieved by retrieving the coordinates from each bounding box and creating a Rectangle object.We can also draw a string with the class label and confidence.\n","\n","The draw_boxes() function below implements this, taking the filename of the original photograph and the parallel lists of bounding boxes, labels and scores, and creates a plot showing all detected objects."]},{"cell_type":"code","metadata":{"id":"pF1kJOxJXl2z"},"source":["# draw all results\n","def draw_boxes(filename, v_boxes, v_labels, v_scores):\n","\t# load the image\n","\tpyplot.figure(figsize=(15, 15))\n","\tdata = pyplot.imread(filename)\n","\t# plot the image\n","\tpyplot.imshow(data)\n","\t# get the context for drawing boxes\n","\tax = pyplot.gca()\n","\t# plot each box\n","\tfor i in range(len(v_boxes)):\n","\t\tbox = v_boxes[i]\n","\t\t# get coordinates\n","\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n","\t\t# calculate width and height of the box\n","\t\twidth, height = x2 - x1, y2 - y1\n","\t\t# create the shape\n","\t\trect = Rectangle((x1, y1), width, height, fill=False, color='black')\n","\t\t# draw the box\n","\t\tax.add_patch(rect)\n","\t\t# draw text and score in top left corner\n","\t\tlabel = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n","\t\tpyplot.text(x1, y1, label, color='black', size='xx-large')\n","\t# show the plot\n","\tpyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRxZ6SZyXxOv"},"source":["# summarize what we found\n","for i in range(len(v_boxes)):\n","\tprint(v_labels[i], v_scores[i])\n","# draw what we found\n","draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxEWemBqYsAX"},"source":[""],"execution_count":null,"outputs":[]}]}